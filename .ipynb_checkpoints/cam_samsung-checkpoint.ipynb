{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot():\n",
    "    pdb.set_trace()\n",
    "    plot_sample = 100\n",
    "    batch_xs = mnist.test.images[:100].reshape(-1,28,28,1)\n",
    "    batch_ys = mnist.test.labels[:100]\n",
    "\n",
    "    labels = sess.run(model,\n",
    "                      feed_dict={X: batch_xs,\n",
    "                                 Y: batch_ys,\n",
    "                                 keep_prob:1})\n",
    "\n",
    "    shuff_idx = np.random.randint(batch_xs.shape[0], size=10)\n",
    "    fig = plt.figure()\n",
    "    for i in range(10):\n",
    "            idx = shuff_idx[i]\n",
    "            subplot = fig.add_subplot(2, 5, i + 1)\n",
    "            subplot.set_xticks([])\n",
    "            subplot.set_yticks([])\n",
    "            subplot.set_title('%d' % np.argmax(labels[idx])) # prediction\n",
    "            subplot.imshow(mnist.test.images[idx].reshape((28, 28)),\n",
    "                          cmap=plt.cm.gray_r)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam():\n",
    " \n",
    "    batch_xs = mnist.test.images[:100].reshape(-1,28,28,1)\n",
    "    batch_ys = mnist.test.labels[:100]\n",
    "    shuff_idx = np.random.randint(batch_xs.shape[0], size=10)\n",
    "    labels, feature_tensor = sess.run([model, L4],\n",
    "                                     feed_dict={X: batch_xs,\n",
    "                                                Y: batch_ys,\n",
    "                                                keep_prob:1})\n",
    "\n",
    "    ##########################################################################\n",
    "    # TODO: Mission 5)                                                       #\n",
    "    #   From the trained model, you could extract class activation map.      #\n",
    "    #   Pick a set of feature tensor and also pick a matching label weight ! #\n",
    "    #   You should pay attention to index of tensor and label.               # \n",
    "    ########################################################################## \n",
    "    # TODO 5-1)                                                              #\n",
    "    # We get random index in 'shuff_idx'                                     #\n",
    "    # You should pick a matching tensor using 'shuff_idx'                    #\n",
    "    ##########################################################################\n",
    "\n",
    "    shuff_idx = np.random.randint(batch_xs.shape[0], size=10)\n",
    "    picked = feature_tensor[shuff_idx]\n",
    "    picked_labels = np.asarray(labels[shuff_idx], dtype=np.float32)\n",
    "    picked_labels = np.argmax(pikcked_labels, axis=1)\n",
    "    ########################################################################## \n",
    "\n",
    "    ####################################################################################### \n",
    "    # TODO 5-2)                                                                           #\n",
    "    # From the class-weight(W5), pick a weight vector corresponding to 'picked_labels'    #\n",
    "    # Hint: .eval(session=sess) would make tensor to numpy array                          #\n",
    "    ####################################################################################### \n",
    "\n",
    "    W = W5.eval(session=sess)\n",
    "    matching_weight = W[:, picked_labels]\n",
    "    matching_weight = np.reshape(matching_weight, (10, 1, 1, 1024))\n",
    "    output = (picked * matching_weight).sum(axis=3)\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    cnt = 1\n",
    "    for i in range(10):\n",
    "        idx = shuff_idx[i]\n",
    "        if i == 5:\n",
    "            cnt = 6\n",
    "        subplot = fig.add_subplot(4, 5, i + cnt)\n",
    "        subplot.set_xticks([])\n",
    "        subplot.set_yticks([])\n",
    "        subplot.set_title('%d' % np.argmax(picked_labels[i])) # prediction\n",
    "        maxer = output[i].max()\n",
    "        miner = output[i].min()\n",
    "        cam = (output[i] - miner) / (maxer - miner)\n",
    "        cam = np.uint8(255*cam)\n",
    "        cam = cv2.resize(cam, (28,28))\n",
    "        heated = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "        orig_img = mnist.test.images[idx].reshape((28,28))\n",
    "        colored = cv2.cvtColor(orig_img, cv2.COLOR_GRAY2RGB)\n",
    "        img = heated * 0.8 + colored * 0.2\n",
    "        img = np.asarray(img, dtype=np.uint8)\n",
    "        subplot.imshow(colored)#cmap=plt.cm.gray_r)\n",
    "        subplot = fig.add_subplot(4,5,i+cnt+5)\n",
    "        subplot.imshow(img)\n",
    "\n",
    "    plt.savefig('cam_result.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# TODO: Mission 1)  ##############################################################################\n",
    "#   You are going to build a feature extractor which is for Class Activation Map.                #\n",
    "#   MNIST dataset is not that large dataset, so you can stack a few layers.                      # \n",
    "#   First, define a placeholder for MNIST input and labels.                                      #\n",
    "#   You may care about output dimension !                                                        #\n",
    "#   Hint 1) tf.Varaible, tf.random_normal, tf.nn.relu, tf.nn.conv2d ...                          #\n",
    "#   Hint 2) The size of MNIST data is (28, 28) and has 10 labels ( 0 ~ 9 )                       # \n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "X = tf.placeholder()\n",
    "Y = tf.placeholder()\n",
    "\n",
    "\n",
    "W1 = tf.Variable()\n",
    "L1 = tf.nn.conv2d()\n",
    "L1 =\n",
    "\n",
    "... \n",
    "\n",
    "\n",
    "\n",
    "# TODO: Mission 2)  ##############################################################################\n",
    "#   The core idea of the Class Activation Map(CAM) is Global Average Pooling (GAP)               # \n",
    "#   Global Average Pooling is the operation aggregating tensor score in channel-wise.            #\n",
    "#   You should make GAP layer and complete the following architecture. Refer to figure on slide. # \n",
    "#   'model' parameter is for your model prediction.                                              # \n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "gbl_avg = tf.reduce_mean() ... \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Mission 4)  ##############################################################################\n",
    "#   The system without regularization loss will perform good on classification.                  #\n",
    "#   But the output of CAM is not that good.                                                      #\n",
    "#   Let's add a regularization term!                                                             #\n",
    "#   You may use l2-loss term.                                                                    #\n",
    "#   Hint: tf.nn.l2_loss                                                                          #   \n",
    "##################################################################################################\n",
    "\n",
    "w1_reg = \n",
    "beta = 2e-4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Mission 3)  ##############################################################################\n",
    "#   Define an opitmizer for training !                                                           #\n",
    "#   You should take 2-steps as follows.                                                          # \n",
    "#       1) Calculate cost with cross-entropy-loss function.                                      #\n",
    "#       2) Choose an optimizer for minimizing your cost.                                         # \n",
    "#   Hint : tf.reduce_mean, tf.nn.softmax_cross_entropy_with_logits                               #\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(...)\n",
    "optimizer =\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "config.log_device_placement = False\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# TODO: \n",
    "batch_size = 10\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# Save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_cost = 0\n",
    "    print('{} epoch ! '.format(epoch))\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "        _, cost_val = sess.run([optimizer, cost],\n",
    "                               feed_dict={X: batch_xs,\n",
    "                                          Y: batch_ys,\n",
    "                                          keep_prob: 0.7})\n",
    "        #print(cost_val)\n",
    "        total_cost += cost_val\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'Avg. cost =', '{:.3f}'.format(total_cost / total_batch))\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "num_test = mnist.test.images.shape[0]\n",
    "batch_size =  100\n",
    "total_batch = int(num_test / batch_size) \n",
    "idx = 0\n",
    "test_acc = 0 \n",
    "for i in range(total_batch):\n",
    "    batch_xs = mnist.test.images[idx:idx+batch_size].reshape(-1,28,28,1)\n",
    "    batch_ys = mnist.test.labels[idx:idx+batch_size]\n",
    "    acc = sess.run(accuracy, feed_dict={X: batch_xs, Y:batch_ys, keep_prob:1})\n",
    "    test_acc += acc \n",
    "    idx += batch_size \n",
    "\n",
    "test_acc /= total_batch\n",
    "print(' The test accuracy is : {} %'.format(test_acc))\n",
    "\n",
    "\n",
    "\n",
    "#plot()\n",
    "#cam()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
